%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Welcome to writeLaTeX --- just edit your LaTeX on the left,
% and we'll compile it for you on the right. If you give
% someone the link to this page, they can edit at the same
% time. See the help menu above for more info. Enjoy!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{cancel}

\setlist[enumerate,1]{label={(\alph*)}} %this changes enumerate to (a),(b),...

\usepackage{graphicx} %package to manage images

\newcommand{\A}{{\mathcal{A}}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\CC}{{\mathcal{C}}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\Z}{{\mathbb Z}}

\newcommand{\Aut}{{\rm Aut}}
\newcommand{\End}{{\rm End}}
\newcommand{\Hom}{{\rm Hom}}
\newcommand{\id}{{\rm id}}
\newcommand{\Ima}{{\rm Im}}
\newcommand{\Ker}{{\rm Ker}}
\newcommand{\Mor}{{\rm Mor}}
\newcommand{\Rad}{{\rm Rad}}
\newcommand{\Prob}{{\sf P}}
\newcommand{\E}{{\sf E}}
\newcommand{\Var}{{\sf Var}}

\renewcommand\labelitemi{-} %this changes itemize bullet points to dashes (-)

\usepackage{listings}
\usepackage{xcolor}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}
{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}
{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}
{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}
{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}
{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}
{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Homework 5}%replace X with the appropriate number
\author{Mengxiang Jiang\\ %replace with your name
Stat 610 Distribution Theory} %if necessary, replace with your course title
 
\maketitle

\begin{problem}{1}
  Suppose $Z$ has standard normal distribution.
  \begin{enumerate}
    \item Use the mgf to find the third and fourth moments of $Z$. 
    (Recall Slide 119.)
    \item Use part (a) to deduce $\E(X^3)$ and $\E(X^4)$ where 
    $X \sim \text{normal}(\mu, \sigma^2)$ (e.g., $X = \mu + \sigma Z$).
  \end{enumerate}
  
  \begin{enumerate}
    \item The mgf of $Z$ is
    \[
      M_Z(t) = \E(e^{tZ}) = e^{\frac{t^2}{2}}.
    \]
    Thus, we take derivatives of $M_Z(t)$:
    \[
      M_Z^{(1)}(t) = t e^{\frac{t^2}{2}},
    \]
    \[
      M_Z^{(2)}(t) = (1 + t^2) e^{\frac{t^2}{2}},
    \]
    \[
      M_Z^{(3)}(t) = (3t + t^3) e^{\frac{t^2}{2}},
    \]
    \[
      M_Z^{(4)}(t) = (3 + 6t^2 + t^4) e^{\frac{t^2}{2}}.
    \]
    Evaluating these derivatives at $t = 0$, we have
    \[
      \E(Z) = M_Z^{(1)}(0) = 0,
    \]
    \[
      \E(Z^2) = M_Z^{(2)}(0) = 1,
    \]
    \[
      \E(Z^3) = M_Z^{(3)}(0) = 0,
    \]
    \[
      \E(Z^4) = M_Z^{(4)}(0) = 3.
    \]
    \item Since $X = \mu + \sigma Z$, we have
    \[
      \begin{aligned}
      \E(X^3) &= \E((\mu + \sigma Z)^3) \\
      &= \mu^3 + 3\mu^2\sigma \E(Z) + 3\mu\sigma^2 \E(Z^2) + \sigma^3 \E(Z^3) \\
      &= \mu^3 + 3\mu\sigma^2,
      \end{aligned}
    \]
    \[
      \begin{aligned}
        \E(X^4) &= \E((\mu + \sigma Z)^4) \\
        &= \E(\mu^4 + 4\mu^3\sigma Z + 6\mu^2\sigma^2 Z^2 
        + 4\mu\sigma^3 Z^3 + \sigma^4 Z^4) \\
        &= \mu^4 + 4\mu^3\sigma \E(Z) + 6\mu^2\sigma^2 \E(Z^2) 
        + 4\mu\sigma^3 \E(Z^3) + \sigma^4 \E(Z^4) \\
        &= \mu^4 + 6\mu^2\sigma^2 + 3\sigma^4.
      \end{aligned}
    \]
  \end{enumerate}
\end{problem}

\begin{problem}{2}
  \textit{Statistical Inference} by Casella and Berger, 2nd Edition, Chapter 2, 
  Exercise 14.
  \begin{itemize}
    \item [14.] \begin{enumerate}
      \item Let $X$ be a continuous, nonnegative random variable
        $[f(x) = 0$ for $x <0]$. Show that
        \[
          \E(X) = \int_0^\infty [1- F_X(x)]dx,
        \]
        where $F_X(x)$ is the cdf of $X$.
      \item Let $X$ be a discrete random variable whose range is
      the nonnegative integers. Show that
      \[
        \E(X) = \sum_{k=0}^\infty [1- F_X(k)],
      \]
      where $F_X(k) = P(X \leq k)$. Compare this with part (a).
      \item Observe that
      \[
        X = \int_0^Xdx = \int_0^\infty 1_{X>x} dx,
      \]
      and then, after taking expectation of the right-
      hand expression, exchange expectation and integral to prove that the 
      expression in part (a) holds for any nonnegative random variable, 
      regardless of the type of distribution. [Exchanging integration and 
      expectation is like exchanging integration and sum or doing
      double integration in the other order, etc. The general result is
      \textit{Fubini's Theorem} and
      is valid, at least, for nonnegative quantities like the example here.]
    \end{enumerate}
    \begin{enumerate}
      \item Since $X$ is a continuous, nonnegative random variable, we have
      \[
        \begin{aligned}
          \E(X) &= \int_0^\infty x f(x) dx \\
          &= \int_0^\infty \int_0^x f(x) dt dx \\
          &= \int_0^\infty \int_t^\infty f(x) dx dt \\
          &= \int_0^\infty \Prob(X > t) dt \\
          &= \int_0^\infty [1 - \Prob(X \leq t)] dt\\
          &= \int_0^\infty [1 - F_X(t)] dt.
        \end{aligned}
      \]
      
      \item Since $X$ is a discrete, nonnegative random variable, we have 
      \[
        \begin{aligned}
          \E(X) &= \sum_{k=0}^\infty k \Prob(X = k) \\
          &= \sum_{k=0}^\infty \sum_{j=1}^k \Prob(X = k) \\
          &= \sum_{j=1}^\infty \sum_{k=j}^\infty \Prob(X = k) \\
          &= \sum_{j=1}^\infty \Prob(X \geq j) \\
          &= \sum_{j=1}^\infty [1 - \Prob(X < j)] \\
          &= \sum_{j=1}^\infty [1 - F_X(j-1)] \\
          &= \sum_{i=0}^\infty [1 - F_X(i)].
        \end{aligned}
      \]

      \item Using the identity given, we have
      \[
        \begin{aligned}
          \E(X) &= \E\left(\int_0^\infty 1_{X > x} dx\right) \\
          &= \int_0^\infty \E(1_{X > x}) dx \\
          &= \int_0^\infty \Prob(X > x) dx \\
          &= \int_0^\infty [1 - \Prob(X \leq x)] dx \\
          &= \int_0^\infty [1 - F_X(x)] dx.
        \end{aligned}
      \]
    \end{enumerate}
  \end{itemize}
\end{problem}

\begin{problem}{3}
  Let $X$ have the \textit{Laplace distribution} 
  (recalling Problem 3 of Assignment 4), with pdf
  \[
    f(x) = \frac{\lambda}{2} e^{-\lambda|x|}, \quad \text{all } x.
  \]
  \begin{enumerate}
    \item Show that $X$ has mgf 
    \[
      M_X (t) = \frac{1}{1-\frac{t^2}{\lambda^2}} 
      \quad \text{for } |t| < \lambda.
    \] 
    Hint: integrate separately for $x < 0$ and $x > 0$, and then combine.
    \item Determine the quantile function for $X$. 
    Again, you need to think about separate cases. 
    Make sure you get a continuous increasing function.
    \item Find the median and the 25-th and 75-th percentiles, 
    as functions of $\lambda$.
  \end{enumerate}
  
  \begin{enumerate}
    \item \[
      \begin{aligned}
        M_X(t) &= \E(e^{tX}) \\
        &= \int_{-\infty}^0 e^{tx} \frac{\lambda}{2} e^{\lambda x} dx 
        + \int_0^\infty e^{tx} \frac{\lambda}{2} e^{-\lambda x} dx \\
        &= \frac{\lambda}{2} \int_{-\infty}^0 e^{(t+\lambda)x} dx 
        + \frac{\lambda}{2} \int_0^\infty e^{(t-\lambda)x} dx \\
        &= \frac{\lambda}{2(t+\lambda)} + \frac{\lambda}{2(\lambda - t)} \\
        &= \frac{\lambda^2}{\lambda^2 - t^2}, \quad |t| < \lambda.
      \end{aligned}
    \]
    \item \[
      F_X(x) = \begin{cases}
        \frac{1}{2} e^{\lambda x}, & x < 0, \\
        1 - \frac{1}{2} e^{-\lambda x}, & x \geq 0.
      \end{cases}
    \]
    By Theorem 2.35, the quantile function is
    \[
      Q_X(p) = F_X^{-1}(p) = \begin{cases}
        \frac{1}{\lambda} \ln(2p), & 0 < p < \frac{1}{2}, \\
        -\frac{1}{\lambda} \ln[2(1-p)], & \frac{1}{2} \leq p < 1.
      \end{cases} 
    \]
    \item The median is
    \[
      F_X^{-1}\left(\frac{1}{2}\right) = \frac{1}{\lambda} \ln(1) = 0.
    \]
    The 25-th percentile is
    \[
      F_X^{-1}\left(\frac{1}{4}\right) 
      = \frac{1}{\lambda} \ln\left(\frac{1}{2}\right) 
      = -\frac{\ln 2}{\lambda}.
    \]
    The 75-th percentile is
    \[
      F_X^{-1}\left(\frac{3}{4}\right) 
      = -\frac{1}{\lambda} \ln\left(\frac{1}{2}\right) 
      = \frac{\ln 2}{\lambda}.
    \]
  \end{enumerate}

\end{problem}

\begin{problem}{4}
  Suppose that the probability of being able to make a left turn 
  on the first signal cycle of a very busy intersection is 32\%. 
  Assuming independent trips, let $W$ be the number of times
  that one is not successful turning on the first cycle before 
  the fifth time that one is successful.
  \begin{enumerate}
    \item What is the distribution of $W$ and its mean and variance?
    \item Determine the chance that $W$ is no more than 10.
    \item Let $Y$ be the number of successes in 15 trips. 
    What is the chance that $Y \geq 5$?
  \end{enumerate}
  \begin{enumerate}
    \item Since $W$ is the number of failures before the fifth success,
    $W$ has a negative binomial distribution with parameters $r = 5$ and $p = 0.32$.
    The mean and variance of a negative binomial distribution are given by
    \[
      \E(W) = \frac{r(1-p)}{p} = \frac{5(1-0.32)}{0.32} 
      = \frac{5 \cdot 0.68}{0.32} = 10.625,
    \]
    \[
      \Var(W) = \frac{r(1-p)}{p^2} = \frac{5(1-0.32)}{0.32^2} 
      = \frac{5 \cdot 0.68}{0.1024} \approx 33.203.
    \]
    \item We want to find $\Prob(W \leq 10)$. Using the negative binomial pmf, 
    we have
    \[
      \begin{aligned}
        \Prob(W \leq 10) &= \sum_{w=0}^{10} \Prob(W = w) \\
        &= \sum_{w=0}^{10} \binom{w+5-1}{5-1} (0.32)^5 (0.68)^w \\
        &= (0.32)^5 \sum_{w=0}^{10} \binom{w+4}{4} (0.68)^w.
      \end{aligned}
    \]
    Using R, we find that $\Prob(W \leq 10) \approx 0.552$.

    \item Since $Y$ is the number of successes in 15 trips,
    $Y$ has a binomial distribution with parameters $n = 15$ and $p = 0.32$.
    We want to find $\Prob(Y \geq 5)$. Using the binomial pmf, we have
    \[
      \begin{aligned}
        \Prob(Y \geq 5) &= 1 - \Prob(Y \leq 4) \\
        &= 1 - \sum_{y=0}^{4} \Prob(Y = y) \\
        &= 1 - \sum_{y=0}^{4} \binom{15}{y} (0.32)^y (0.68)^{15-y}.
      \end{aligned}
    \]
    Using R, we find that $\Prob(Y \geq 5) \approx 0.552$.
  \end{enumerate}
\end{problem}

\begin{problem} {5}
  \textit{Statistical Inference} by Casella and Berger, 2nd Edition, 
  Chapter 3, Exercise 24(b).
  \begin{itemize}
    \item [24.] Many ``named'' distributions are special cases of the
    more common distributions already discussed. For each of the following
    named distributions derive the form of the pdf, verify that it is
    a pdf, and calculate the mean and variance.
    \begin{itemize}
      \item [(b)] If $X \sim \text{exponential}(\beta)$, then 
      $Y = (2X/\beta)^{\frac{1}{2}}$ has the \textit{Rayleigh distribution}.
    \end{itemize}
    \begin{itemize}
      \item [(b)] Since $X \sim \text{exponential}(\beta)$, the pdf of $X$ is
      \[
        f_X(x) = \frac{1}{\beta} e^{-\frac{x}{\beta}}, \quad x > 0.
      \]
      The transformation is $Y = g(X) = (2X/\beta)^{\frac{1}{2}}$.
      The inverse transformation is $X = g^{-1}(Y) = \frac{\beta Y^2}{2}$.
      The derivative of the inverse transformation is
      \[
        \frac{d}{dy} g^{-1}(Y) = \beta Y.
      \]
      Thus, the pdf of $Y$ is
      \[
        \begin{aligned}
          f_Y(y) &= f_X(g^{-1}(y)) 
          \left| \frac{d}{dy} g^{-1}(y) \right| \\
          &= f_X\left(\frac{\beta y^2}{2}\right) (\beta y) \\
          &= \frac{1}{\beta} e^{-\frac{\beta y^2/2}{\beta}} (\beta y) \\
          &= y e^{-\frac{y^2}{2}}, \quad y > 0.
        \end{aligned}
      \]
      To verify that $f_Y(y)$ is a pdf, we check that
      \[
        \int_0^\infty f_Y(y) dy 
        = \int_0^\infty y e^{-\frac{y^2}{2}} dy.
      \]
      Let $u = -\frac{y^2}{2}$, then $du = -y dy$. Thus,
      \[
        \int_0^\infty y e^{-\frac{y^2}{2}} dy 
        = -\int_0^{-\infty} e^u du 
        = -[e^u]_0^{-\infty} 
        = 1.
      \]
      Therefore, $f_Y(y)$ is a valid pdf.

      Next, we calculate the mean of $Y$:
      \[
        \begin{aligned}
          \E(Y) &= \int_0^\infty y
          f_Y(y) dy \\
          &= \int_0^\infty y^2 e^{-\frac{y^2}{2}} dy.
        \end{aligned}
      \]
      Using the gamma function, we have
      \[
        \begin{aligned}
          \E(Y) &= \int_0^\infty y^2 e^{-\frac{y^2}{2}} dy \\
          &= 2^{\frac{3}{2}-1} \int_0^\infty t^{\frac{3}{2}-1} e^{-t} dt \\
          &= 2^{\frac{3}{2}-1} \Gamma\left(\frac{3}{2}\right) \\
          &= \sqrt{2} \cdot \frac{1}{2} \sqrt{\pi} \\
          &= \sqrt{\frac{\pi}{2}}.
        \end{aligned}
      \]
      Finally, we calculate the variance of $Y$:
      \[
        \begin{aligned}
          \E(Y^2) &= \int_0^\infty y^2 f_Y(y) dy \\
          &= \int_0^\infty y^3 e^{-\frac{y^2}{2}} dy \\
          &= 2^{\frac{4}{2}-1} \int_0^\infty t^{\frac{4}{2}-1} e^{-t} dt \\
          &= 2^{\frac{4}{2}-1} \Gamma\left(\frac{4}{2}\right) \\
          &= 2^{\frac{4}{2}-1} \cdot 1! \\
          &= 2.
        \end{aligned}
      \]
      Thus,
      \[
        \Var(Y) = \E(Y^2) - [\E(Y)]^2 
        = 2 - \frac{\pi}{2} = \frac{4 - \pi}{2}.
      \]
  \end{itemize}
\end{itemize}
\end{problem}

\begin{problem}{6}
Recall Theorem 3.4 in the notes.
  \begin{enumerate}
    \item Analytically prove that
    \[
      \int_0^t \frac{\lambda^n u^{n-1} e^{-\lambda u}}{(n-1)!} du 
      = 1 - \sum_{j=0}^{n-1} \frac{(\lambda t)^j e^{-\lambda t}}{j!}, 
      \quad t \geq 0,
    \]
    by showing (i) that both sides have the same derivative with 
    respect to $t$ and (ii) that both have the same value when $t = 0$.
    \item Let $\lambda = 2.5$, $n = 5$ and $t = 3$, and use the 
    \texttt{ppois} and \texttt{pgamma} functions in R (the Poisson
    and gamma cdfs, respectively) to find $\Prob(Y \geq 5)$ and 
    $\Prob(T \leq 3)$ for $Y \sim \text{Poisson}(7.5)$ and
    $T \sim \text{gamma}(5, 0.4)$.
  \end{enumerate}
  \begin{theorem}{3.4}
    Let $X_k$ be the time until the $k$-th occurrence for a Poisson
    process and let $Y_t$ be the number of occurrences in the 
    interval $[0, t]$. Then $X_k \sim \text{gamma}(k, 1/\lambda)$ and 
    $Y_t \sim \text{Poisson}(\lambda t)$. Furthermore,
    $\Prob(X_k \leq t) = \Prob(Y_t \geq k)$.
    Specifically,
    \[
      \int_0^t \frac{\lambda^k u^{k-1} e^{-\lambda u}}{(k-1)!} du = 
      1 - \sum_{j=0}^{k-1} \frac{(\lambda t)^j e^{-\lambda t}}{j!}, 
      \quad t \geq 0.
    \]
  \end{theorem}
  \begin{enumerate}
    \item Let
    \[
      L(t) = \int_0^t \frac{\lambda^n u^{n-1} e^{-\lambda u}}{(n-1)!} du,
    \]
    \[
      R(t) = 1 - \sum_{j=0}^{n-1} \frac{(\lambda t)^j e^{-\lambda t}}{j!}.
    \]
    Then,
    \[
      \begin{aligned}
        L'(t) &= \frac{\lambda^n t^{n-1} e^{-\lambda t}}{(n-1)!}, \\
        R'(t) &= -\sum_{j=0}^{n-1} \left[ \frac{j(\lambda t)^{j-1} e^{-\lambda t}}{j!} 
        + \frac{(\lambda t)^j (-\lambda) e^{-\lambda t}}{j!} \right] \\
        &= -\sum_{j=0}^{n-1} \left[ \frac{\lambda^j t^{j-1} e^{-\lambda t}}{(j-1)!} 
        - \frac{\lambda^{j+1} t^j e^{-\lambda t}}{j!} \right] \\
        &= -\left[ \sum_{j=1}^{n-1} \frac{\lambda^j t^{j-1} e^{-\lambda t}}{(j-1)!} 
        - \sum_{j=0}^{n-2} \frac{\lambda^{j+1} t^j e^{-\lambda t}}{j!} 
        - \frac{\lambda^n t^{n-1} e^{-\lambda t}}{(n-1)!} \right] \\
        &= -\left[ \sum_{i=0}^{n-2} \frac{\lambda^{i+1} t^i e^{-\lambda t}}{i!} 
        - \sum_{i=0}^{n-2} \frac{\lambda^{i+1} t^i e^{-\lambda t}}{i!} 
        - \frac{\lambda^n t^{n-1} e^{-\lambda t}}{(n-1)!} \right] \\
        &= \frac{\lambda^n t^{n-1} e^{-\lambda t}}{(n-1)!}.
      \end{aligned}
    \]
    Thus, $L'(t) = R'(t)$.
    Also, $L(0) = 0$ and $R(0) = 1 - 1 = 0$. Therefore, $L(t) = R(t)$.
    \item Using R, we have
    \[
      \Prob(Y \geq 5) = 1 - \texttt{ppois}(4, 7.5) \approx 0.868,
    \]
    and
    \[
      \Prob(T \leq 3) = \texttt{pgamma}(3, 5, 0.4) \approx 0.868.
    \]
  \end{enumerate}
\end{problem}

\begin{problem}{7}
  Let $Z \sim \text{normal}(0, 1)$. Prove $Z^2 \sim \chi^2(1)$. This is 
  Theorem 3.11 in the notes. Hint: express the event $Z^2 \leq y$ 
  as an interval of values for $Z$, keeping in mind that $Z$ can be
  negative and positive.
  \begin{theorem}{3.11}
    Let $Z \sim \text{normal}(0, 1)$. Then $Z^2 \sim \chi^2(1)$.
  \end{theorem}
  Let $Y = Z^2$. Then, for $y \geq 0$,
  \[
    \begin{aligned}
      F_Y(y) &= \Prob(Y \leq y) \\
      &= \Prob(Z^2 \leq y) \\
      &= \Prob(-\sqrt{y} \leq Z \leq \sqrt{y}) \\
      &= F_Z(\sqrt{y}) - F_Z(-\sqrt{y}) \\
      &= F_Z(\sqrt{y}) - [1 - F_Z(\sqrt{y})] \\
      &= 2F_Z(\sqrt{y}) - 1.
    \end{aligned}
  \]
  Thus, the pdf of $Y$ is
  \[
    \begin{aligned}
      f_Y(y) &= F_Y'(y) \\
      &= 2 f_Z(\sqrt{y}) \cdot \frac{1}{2\sqrt{y}} \\
      &= \frac{1}{\sqrt{2\pi y}} e^{-\frac{y}{2}}, \quad y > 0.
    \end{aligned}
  \]
  This is the pdf of a $\chi^2(1)$ distribution. Therefore, $Z^2 \sim \chi^2(1)$.
\end{problem}

% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}
